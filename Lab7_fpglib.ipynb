{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3aac5af-225c-4e4b-9881-c3fe1ba3acc9",
      "metadata": {
        "id": "f3aac5af-225c-4e4b-9881-c3fe1ba3acc9",
        "outputId": "8ae110c0-63f6-44d5-adb5-222165a76ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time required for apriori :  0.006939411163330078\n",
            "   support   itemsets\n",
            "0     0.75        (A)\n",
            "1     0.75        (B)\n",
            "2     0.75        (C)\n",
            "3     0.50        (D)\n",
            "4     0.50     (B, A)\n",
            "5     0.75     (A, C)\n",
            "6     0.50     (B, C)\n",
            "7     0.50  (B, A, C)\n"
          ]
        }
      ],
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Sample dataset (list of transactions)\n",
        "dataset = pd.DataFrame([['A', 'B', 'C'],\n",
        "           ['D', 'B'],\n",
        "           ['B', 'C', 'D', 'A'],\n",
        "           ['B', 'A', 'C'],\n",
        "                       []])\n",
        "\n",
        "start=time.time()\n",
        "# Apply Apriori algorithm to get frequent itemsets\n",
        "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
        "end=time.time()\n",
        "print(\"Time required for apriori : \",end-start)\n",
        "\n",
        "# Display the result\n",
        "print(frequent_itemsets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6337fef9-3a7d-45ce-a6d7-56246feeb81b",
      "metadata": {
        "id": "6337fef9-3a7d-45ce-a6d7-56246feeb81b",
        "outputId": "dd9eaadc-95bd-4a80-c9a3-d4ce10d562de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time required for apriori :  0.001711130142211914\n",
            "   support   itemsets\n",
            "0     0.75        (C)\n",
            "1     0.75        (B)\n",
            "2     0.75        (A)\n",
            "3     0.50        (D)\n",
            "4     0.50     (B, C)\n",
            "5     0.75     (A, C)\n",
            "6     0.50     (B, A)\n",
            "7     0.50  (B, A, C)\n"
          ]
        }
      ],
      "source": [
        "from mlxtend.frequent_patterns import fpgrowth\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Sample dataset (list of transactions)\n",
        "dataset = pd.DataFrame([['A', 'B', 'C'],\n",
        "           ['A', 'C', 'B', 'D'],\n",
        "           ['A', 'C', 'B', 'C'],\n",
        "           ['A', 'D', 'B'],\n",
        "           ['B', 'A', 'C']])\n",
        "\n",
        "start2=time.time()\n",
        "# Apply Apriori algorithm to get frequent itemsets\n",
        "frequent_itemsets = fpgrowth(df, min_support=0.5, use_colnames=True)\n",
        "end2=time.time()\n",
        "print(\"Time required for apriori : \",end2-start2)\n",
        "\n",
        "# Display the result\n",
        "print(frequent_itemsets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f8fdae-74cc-4769-8d3b-2a5ee55b3384",
      "metadata": {
        "id": "71f8fdae-74cc-4769-8d3b-2a5ee55b3384",
        "outputId": "e03d1faa-df75-4e09-d02f-2fd228c70b74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemset\t\t Support\n",
            "({'C'}, 0.6666666666666666)\n",
            "({'A', 'C'}, 1.0)\n",
            "({'B', 'A', 'C'}, 1.0)\n",
            "({'B', 'C'}, 2.0)\n",
            "({'D'}, 0.6666666666666666)\n",
            "({'D', 'A'}, 0.6666666666666666)\n",
            "({'D', 'B', 'A'}, 0.6666666666666666)\n",
            "({'D', 'B'}, 1.6666666666666667)\n",
            "({'B'}, 1.0)\n",
            "({'A'}, 1.0)\n",
            "({'B', 'A'}, 1.0)\n"
          ]
        }
      ],
      "source": [
        "def mine_tree(node, header_table, min_support, prefix, frequent_item_list, dataset_size):\n",
        "    sorted_items = [item[0] for item in sorted(header_table.items(), key=lambda p: p[1][0])]\n",
        "    for item in sorted_items:\n",
        "        new_freq_set = prefix.copy()\n",
        "        new_freq_set.add(item)\n",
        "        support = header_table[item][0] / dataset_size\n",
        "        frequent_item_list.append((new_freq_set, support))\n",
        "        cond_patt_bases = find_prefix_path(header_table, item)\n",
        "        cond_tree, cond_header_table = construct_tree(cond_patt_bases, min_support)\n",
        "        if cond_header_table is not None and len(cond_header_table) > 0:\n",
        "            mine_tree(cond_tree, cond_header_table, min_support, new_freq_set, frequent_item_list, dataset_size)\n",
        "\n",
        "def fpgrowth(dataset, min_support):\n",
        "    root, header_table = construct_tree(dataset, min_support)\n",
        "    frequent_item_list = []\n",
        "    dataset_size = sum([dataset[item] for item in dataset])\n",
        "    mine_tree(root, header_table, min_support, set(), frequent_item_list, dataset_size)\n",
        "    return frequent_item_list\n",
        "\n",
        "# Sample transaction data\n",
        "dataset = {\n",
        "    frozenset({'A', 'B', 'C'}): 1,\n",
        "    frozenset({'A', 'C', 'B', 'D'}): 1,\n",
        "    frozenset({'B', 'C', 'D', 'A'}): 1,\n",
        "    frozenset({'B', 'A', 'C'}): 1,\n",
        "    frozenset({'A', 'D', 'B'}): 1,\n",
        "}\n",
        "\n",
        "# Apply FP-Growth algorithm\n",
        "min_support = 2\n",
        "frequent_itemsets = fpgrowth(dataset, min_support)\n",
        "print(\"Itemset\\t\\t Support\")\n",
        "\n",
        "for itemset in frequent_itemsets:\n",
        "    print(itemset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9232eda4-725c-4e48-baef-80a5610b0598",
      "metadata": {
        "id": "9232eda4-725c-4e48-baef-80a5610b0598",
        "outputId": "455c0a33-7e83-4adf-f438-c59caca8d5a0"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'groceries_subset.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 141\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current_node\u001b[38;5;241m.\u001b[39mchildren[item]\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m     transactions \u001b[38;5;241m=\u001b[39m \u001b[43mload_transactions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroceries_subset.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     min_support \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(transactions)\n\u001b[1;32m    143\u001b[0m     frequent_patterns \u001b[38;5;241m=\u001b[39m fp_growth(transactions, min_support)\n",
            "Cell \u001b[0;32mIn[45], line 84\u001b[0m, in \u001b[0;36mload_transactions\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_transactions\u001b[39m(file_path):\n\u001b[1;32m     83\u001b[0m     transactions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     85\u001b[0m         reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file)\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'groceries_subset.csv'"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "class FPNode:\n",
        "    def __init__(self, item, count, parent):\n",
        "        self.item = item\n",
        "        self.count = count\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.next_sibling = None\n",
        "\n",
        "def conditional_tree_from_paths(paths, min_support):\n",
        "    items = defaultdict(int)\n",
        "    for path in paths:\n",
        "        for node in path:\n",
        "            items[node.item] += node.count\n",
        "\n",
        "    items = {item: count for item, count in items.items() if count >= min_support}\n",
        "\n",
        "\n",
        "    frequent_items = sorted(items.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if len(frequent_items) == 0:\n",
        "        return None, None\n",
        "\n",
        "    # root of the conditional tree\n",
        "    root = FPNode(None, None, None)\n",
        "    header_table = {}\n",
        "\n",
        "    for item, count in frequent_items:\n",
        "        header_table[item] = [count, None]\n",
        "\n",
        "    for path in paths:\n",
        "        path = [(node.item, node.count) for node in path if node.item in items]\n",
        "        current_node = root\n",
        "        for item, count in path:\n",
        "            if item in current_node.children:\n",
        "                current_node.children[item].count += count\n",
        "            else:\n",
        "                new_node = FPNode(item, count, current_node)\n",
        "                current_node.children[item] = new_node\n",
        "\n",
        "                if header_table[item][1] is None:\n",
        "                    header_table[item][1] = new_node\n",
        "                else:\n",
        "                    previous_node = header_table[item][1]\n",
        "                    while previous_node.next_sibling is not None:\n",
        "                        previous_node = previous_node.next_sibling\n",
        "                    previous_node.next_sibling = new_node\n",
        "\n",
        "            current_node = current_node.children[item]\n",
        "\n",
        "    return root, header_table\n",
        "\n",
        "def find_prefix_path(node):\n",
        "    path = []\n",
        "    while node is not None:\n",
        "        prefix_path = []\n",
        "        current_node = node\n",
        "        while current_node.parent is not None:\n",
        "            prefix_path.append(current_node)\n",
        "            current_node = current_node.parent\n",
        "        if prefix_path:\n",
        "            path.append(prefix_path)\n",
        "        node = node.next_sibling\n",
        "    return path\n",
        "\n",
        "def mine_frequent_patterns(tree, header_table, min_support, prefix, frequent_patterns):\n",
        "    for item, node in header_table.items():\n",
        "        support = node[0]\n",
        "        if support >= min_support and item not in prefix:\n",
        "            new_freq_set = prefix.copy()\n",
        "            new_freq_set.add(item)\n",
        "            frequent_patterns[tuple(sorted(new_freq_set))] = support\n",
        "\n",
        "            conditional_tree, conditional_header = conditional_tree_from_paths(\n",
        "                find_prefix_path(node[1]), min_support)\n",
        "\n",
        "            if conditional_tree is not None:\n",
        "                mine_frequent_patterns(conditional_tree, conditional_header, min_support, new_freq_set, frequent_patterns)\n",
        "\n",
        "def load_transactions(file_path):\n",
        "    transactions = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            transactions.append(row)\n",
        "    return transactions\n",
        "\n",
        "def fp_growth(transactions, min_support):\n",
        "    # Count the frequency\n",
        "    item_counts = defaultdict(int)\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            item_counts[item] += 1\n",
        "\n",
        "\n",
        "    item_counts = {item: count for item, count in item_counts.items() if count >= min_support}\n",
        "\n",
        "    # Sort items by support count\n",
        "    frequent_items = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    frequent_patterns = {}\n",
        "\n",
        "    # FP-tree and header table\n",
        "    root = FPNode(None, None, None)\n",
        "    header_table = {}\n",
        "\n",
        "    for item, count in frequent_items:\n",
        "        header_table[item] = [count, None]\n",
        "\n",
        "    for transaction in transactions:\n",
        "        current_node = root\n",
        "        for item in transaction:\n",
        "            if item in header_table:\n",
        "                current_node = insert_tree_node(current_node, item, header_table, 1)\n",
        "\n",
        "\n",
        "    mine_frequent_patterns(root, header_table, min_support, set(), frequent_patterns)\n",
        "\n",
        "    return frequent_patterns\n",
        "\n",
        "def insert_tree_node(current_node, item, header_table, count):\n",
        "    if item in current_node.children:\n",
        "        current_node.children[item].count += count\n",
        "    else:\n",
        "        new_node = FPNode(item, count, current_node)\n",
        "        current_node.children[item] = new_node\n",
        "\n",
        "        if header_table[item][1] is None:\n",
        "            header_table[item][1] = new_node\n",
        "        else:\n",
        "            previous_node = header_table[item][1]\n",
        "            while previous_node.next_sibling is not None:\n",
        "                previous_node = previous_node.next_sibling\n",
        "            previous_node.next_sibling = new_node\n",
        "\n",
        "    return current_node.children[item]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    transactions = load_transactions(\"groceries_subset.csv\")\n",
        "    min_support = 0.05 * len(transactions)\n",
        "    frequent_patterns = fp_growth(transactions, min_support)\n",
        "    print(\"Frequent Patterns:\")\n",
        "    for pattern, support in frequent_patterns.items():\n",
        "        print(f\"{pattern}: {support}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2f5bfc-57a0-43bc-9baa-797219d97892",
      "metadata": {
        "id": "ae2f5bfc-57a0-43bc-9baa-797219d97892"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}